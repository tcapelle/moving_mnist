{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.conv_rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Convolutional Kernels\n",
    "> ConvLSTM and ConvGRU cells and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai.vision.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeForce RTX 2070 SUPER\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(1)\n",
    "    print(torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv Coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AddCoords(Module):\n",
    "\n",
    "    def __init__(self, with_r=False):\n",
    "        self.with_r = with_r\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_tensor: shape(batch, channel, x_dim, y_dim)\n",
    "        \"\"\"\n",
    "        batch_size, _, x_dim, y_dim = input_tensor.size()\n",
    "\n",
    "        xx_channel = torch.arange(x_dim).repeat(1, y_dim, 1)\n",
    "        yy_channel = torch.arange(y_dim).repeat(1, x_dim, 1).transpose(1, 2)\n",
    "\n",
    "        xx_channel = xx_channel.float() / (x_dim - 1)\n",
    "        yy_channel = yy_channel.float() / (y_dim - 1)\n",
    "\n",
    "        xx_channel = xx_channel * 2 - 1\n",
    "        yy_channel = yy_channel * 2 - 1\n",
    "\n",
    "        xx_channel = xx_channel.repeat(batch_size, 1, 1, 1).transpose(2, 3)\n",
    "        yy_channel = yy_channel.repeat(batch_size, 1, 1, 1).transpose(2, 3)\n",
    "\n",
    "        ret = torch.cat([\n",
    "            input_tensor,\n",
    "            xx_channel.type_as(input_tensor),\n",
    "            yy_channel.type_as(input_tensor)], dim=1)\n",
    "\n",
    "        if self.with_r:\n",
    "            rr = torch.sqrt(torch.pow(xx_channel.type_as(input_tensor) - 0.5, 2) + torch.pow(yy_channel.type_as(input_tensor) - 0.5, 2))\n",
    "            ret = torch.cat([ret, rr], dim=1)\n",
    "\n",
    "        return ret\n",
    "\n",
    "\n",
    "@delegates(nn.Conv2d)\n",
    "class CoordConv(Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, **kwargs):\n",
    "        self.addcoords = AddCoords(with_r=True)\n",
    "        in_size = in_channels+2\n",
    "        self.conv = nn.Conv2d(in_size+1, out_channels, kernel_size, **kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ret = self.addcoords(x)\n",
    "        ret = self.conv(ret)\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConvGRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ConvGRU_cell(Module):\n",
    "    def __init__(self, in_ch, out_ch, ks=3, debug=False):\n",
    "        self.in_ch = in_ch\n",
    "        # kernel_size of input_to_state equals state_to_state\n",
    "        self.ks = ks\n",
    "        self.out_ch = out_ch\n",
    "        self.debug = debug\n",
    "        self.padding = (ks - 1) // 2\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(self.in_ch + self.out_ch,2 * self.out_ch, self.ks, 1,self.padding),\n",
    "                                   nn.GroupNorm(2 * self.out_ch // 8, 2 * self.out_ch))\n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(self.in_ch + self.out_ch,self.out_ch, self.ks, 1, self.padding),\n",
    "                                   nn.GroupNorm(self.out_ch // 8, self.out_ch))\n",
    "\n",
    "    def forward(self, inputs, hidden_state=None):\n",
    "        \"inputs shape: (bs, seq_len, ch, w, h)\"\n",
    "        bs, seq_len, ch, w, h = inputs.shape\n",
    "        if hidden_state is None:\n",
    "            htprev = self.initHidden(bs, self.out_ch, w, h)\n",
    "            if self.debug: print(f'htprev: {htprev.shape}')\n",
    "        else:\n",
    "            htprev = hidden_state\n",
    "        output_inner = []\n",
    "        for index in range(seq_len):\n",
    "            x = inputs[:, index, ...]\n",
    "            combined_1 = torch.cat((x, htprev), 1)  # X_t + H_t-1\n",
    "            gates = self.conv1(combined_1)  # W * (X_t + H_t-1)          \n",
    "            zgate, rgate = torch.split(gates, self.out_ch, dim=1)\n",
    "            z = torch.sigmoid(zgate)\n",
    "            r = torch.sigmoid(rgate)\n",
    "            combined_2 = torch.cat((x, r * htprev),1)\n",
    "            ht = self.conv2(combined_2)\n",
    "            ht = torch.tanh(ht)\n",
    "            htnext = (1 - z) * htprev + z * ht\n",
    "            output_inner.append(htnext)\n",
    "            htprev = htnext\n",
    "        return torch.stack(output_inner, dim=1), htnext\n",
    "    def __repr__(self): return f'ConvGRU_cell(in={self.in_ch}, out={self.out_ch}, ks={self.ks})'\n",
    "    def initHidden(self, bs, ch, w, h): return one_param(self).new_zeros(bs, ch, w, h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvGRU_cell(in=32, out=32, ks=3)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cell = ConvGRU_cell(32, 32, debug=True)\n",
    "cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "htprev: torch.Size([2, 32, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2, 7, 32, 64, 64)\n",
    "out, h = cell(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 7, 32, 64, 64])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(out.shape, x.shape) \n",
    "test_eq(h.shape, [2,32,64,64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should be possible to call with hidden state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out2, h2 = cell(out, h)\n",
    "test_eq(h2.shape, [2, 32, 64, 64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very nasty module to propagate 2D layers over sequence of images, inspired from Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TimeDistributed(Module):\n",
    "    \"Applies a module over tdim identically for each step\" \n",
    "    def __init__(self, module, low_mem=False, tdim=1):\n",
    "        self.module = module\n",
    "        self.low_mem = low_mem\n",
    "        self.tdim = tdim\n",
    "        \n",
    "    def forward(self, *args, **kwargs):\n",
    "        \"input x with shape:(bs,seq_len,channels,width,height)\"\n",
    "        if self.low_mem or self.tdim!=1: \n",
    "            return self.low_mem_forward(*args)\n",
    "        else:\n",
    "            #only support tdim=1\n",
    "            inp_shape = args[0].shape\n",
    "            bs, seq_len = inp_shape[0], inp_shape[1]   \n",
    "            out = self.module(*[x.view(bs*seq_len, *x.shape[2:]) for x in args], **kwargs)\n",
    "            out_shape = out.shape\n",
    "            return out.view(bs, seq_len,*out_shape[1:])\n",
    "    \n",
    "    def low_mem_forward(self, *args, **kwargs):                                           \n",
    "        \"input x with shape:(bs,seq_len,channels,width,height)\"\n",
    "        tlen = args[0].shape[self.tdim]\n",
    "        args_split = [torch.unbind(x, dim=self.tdim) for x in args]\n",
    "        out = []\n",
    "        for i in range(tlen):\n",
    "            out.append(self.module(*[args[i] for args in args_split]), **kwargs)\n",
    "        return torch.stack(out,dim=self.tdim)\n",
    "    def __repr__(self):\n",
    "        return f'TimeDistributed({self.module})'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dummy(Module):\n",
    "    def __init__(self): pass\n",
    "    def forward(self, x, y): return x+y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeDistributed(Conv2d(2, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdconv = TimeDistributed(nn.Conv2d(2, 5, 3, 1, 1))\n",
    "tdconv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10, 5, 8, 8])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdconv.low_mem_forward(torch.rand(3, 10, 2, 8, 8)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tconv2 = TimeDistributed(Dummy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10, 5])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tconv2.low_mem_forward(torch.rand(3, 10, 5), torch.rand(3, 10, 5)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10, 5, 8, 8])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdconv(torch.rand(3, 10, 2, 8, 8)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Encoder(Module):\n",
    "    def __init__(self, n_in=1, szs=[16,64,96], ks=3, rnn_ks=5, act=nn.ReLU, norm=None, coord_conv=False, debug=False):\n",
    "        self.debug = debug\n",
    "        convs = []\n",
    "        rnns = []\n",
    "        if coord_conv: \n",
    "            self.coord_conv = TimeDistributed(CoordConv(n_in, 8, kernel_size=1))\n",
    "            szs = [8]+szs\n",
    "        else: \n",
    "            self.coord_conv = Lambda(noop)\n",
    "            szs = [n_in]+szs\n",
    "        for ni, nf in zip(szs[0:-1], szs[1:]):\n",
    "            convs.append(ConvLayer(ni, nf, ks=ks, stride=1 if ni==szs[0] else 2, padding=ks//2, act_cls=act, norm_type=norm))\n",
    "            rnns.append(ConvGRU_cell(nf, nf, ks=rnn_ks))\n",
    "        self.convs = nn.ModuleList(TimeDistributed(conv) for conv in convs)\n",
    "        self.rnns = nn.ModuleList(rnns)\n",
    "        \n",
    "    def forward_by_stage(self, inputs, conv, rnn):\n",
    "        if self.debug: \n",
    "            print(f' Layer: {rnn}')\n",
    "            print(' inputs: ', inputs.shape)\n",
    "        inputs = conv(inputs)\n",
    "        if self.debug: print(' after_convs: ', inputs.shape)\n",
    "        outputs_stage, state_stage = rnn(inputs, None)\n",
    "        if self.debug: print(' output_stage: ', outputs_stage.shape)\n",
    "        return outputs_stage, state_stage\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"inputs.shape bs,seq_len,1,64,64\"\n",
    "        hidden_states = []\n",
    "        outputs = []\n",
    "        inputs = self.coord_conv(inputs)\n",
    "        for i, (conv, rnn) in enumerate(zip(self.convs, self.rnns)):\n",
    "            if self.debug: print('stage: ',i)\n",
    "            inputs, state_stage = self.forward_by_stage(inputs, conv, rnn)\n",
    "            outputs.append(inputs)\n",
    "            hidden_states.append(state_stage)\n",
    "        return outputs, hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = Encoder(debug=True, coord_conv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = torch.rand(2, 10, 1, 64, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stage:  0\n",
      " Layer: ConvGRU_cell(in=16, out=16, ks=5)\n",
      " inputs:  torch.Size([2, 10, 8, 64, 64])\n",
      " after_convs:  torch.Size([2, 10, 16, 64, 64])\n",
      " output_stage:  torch.Size([2, 10, 16, 64, 64])\n",
      "stage:  1\n",
      " Layer: ConvGRU_cell(in=64, out=64, ks=5)\n",
      " inputs:  torch.Size([2, 10, 16, 64, 64])\n",
      " after_convs:  torch.Size([2, 10, 64, 32, 32])\n",
      " output_stage:  torch.Size([2, 10, 64, 32, 32])\n",
      "stage:  2\n",
      " Layer: ConvGRU_cell(in=96, out=96, ks=5)\n",
      " inputs:  torch.Size([2, 10, 64, 32, 32])\n",
      " after_convs:  torch.Size([2, 10, 96, 16, 16])\n",
      " output_stage:  torch.Size([2, 10, 96, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "enc_outs, h = enc(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([2, 16, 64, 64]),\n",
       " torch.Size([2, 64, 32, 32]),\n",
       " torch.Size([2, 96, 16, 16])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[_.shape for _ in h]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([2, 10, 16, 64, 64]),\n",
       " torch.Size([2, 10, 64, 32, 32]),\n",
       " torch.Size([2, 10, 96, 16, 16])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[_.shape for _ in enc_outs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "class UpsampleBlock(Module):\n",
    "    \"A quasi-UNet block, using `PixelShuffle_ICNR upsampling`.\"\n",
    "    @delegates(ConvLayer.__init__)\n",
    "    def __init__(self, in_ch, out_ch, residual=True, blur=False, act_cls=defaults.activation,\n",
    "                 self_attention=False, init=nn.init.kaiming_normal_, norm_type=None, debug=False, **kwargs):\n",
    "        store_attr()\n",
    "        self.shuf = PixelShuffle_ICNR(in_ch, in_ch//2, blur=blur, act_cls=act_cls, norm_type=norm_type)\n",
    "        ni = in_ch//2 if not residual else in_ch//2 + out_ch  #the residual has out_ch (normally in_ch//2)\n",
    "        nf = out_ch\n",
    "        self.conv1 = ConvLayer(ni, nf, act_cls=act_cls, norm_type=norm_type, **kwargs)\n",
    "        self.conv2 = ConvLayer(nf, nf, act_cls=act_cls, norm_type=norm_type,\n",
    "                               xtra=SelfAttention(nf) if self_attention else None, **kwargs)\n",
    "        self.bn = nn.BatchNorm2d(out_ch)\n",
    "        self.relu = act_cls()\n",
    "        apply_init(nn.Sequential(self.conv1, self.conv2), init)\n",
    "    def __repr__(self): return (f'UpsampleBLock(in={self.in_ch}, out={self.out_ch}, blur={self.blur}, residual={self.residual}, '\n",
    "                                f'act={self.act_cls()}, attn={self.self_attention}, norm={self.norm_type})')\n",
    "    def forward(self, up_in, side_in=None):\n",
    "        up_out = self.shuf(up_in)\n",
    "        if side_in is not None:\n",
    "            if self.debug: print(f'up_out: {up_out.shape}, side_in: {side_in.shape}')\n",
    "            assert up_out.shape[-2:] == side_in.shape[-2::], 'residual shape does not match input'\n",
    "            up_out = torch.cat([up_out, self.bn(side_in)], dim=1)\n",
    "        if self.debug: print(f'up_out: {up_out.shape}')\n",
    "        return self.conv2(self.conv1(up_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UpsampleBLock(in=32, out=64, blur=False, residual=False, act=ReLU(), attn=False, norm=None)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "us = UpsampleBlock(32, 64, residual=False)\n",
    "us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 64, 64, 64])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "us(torch.rand(8, 32, 32, 32)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UpsampleBLock(in=32, out=64, blur=False, residual=False, act=ReLU(), attn=False, norm=None)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "us_residual = UpsampleBlock(32, 16, residual=True, debug=True)\n",
    "us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "up_out: torch.Size([8, 16, 64, 64]), side_in: torch.Size([8, 16, 64, 64])\n",
      "up_out: torch.Size([8, 32, 64, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 16, 64, 64])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "us_residual(torch.rand(8, 32, 32, 32), side_in=torch.rand(8, 16, 64, 64)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it can take a residual connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Decoder(Module):\n",
    "    def __init__(self, n_out=1, szs=[96,64,16], ks=3, rnn_ks=5, act=nn.ReLU, blur=False, attn=False, \n",
    "                 norm=None, debug=False):\n",
    "        self.debug = debug\n",
    "        deconvs = []\n",
    "        rnns = []\n",
    "        szs = szs\n",
    "        for ni, nf in zip(szs[0:-1], szs[1:]):\n",
    "            deconvs.append(UpsampleBlock(ni, nf, blur=blur, self_attention=attn, act_cls=act, norm_type=norm))\n",
    "            rnns.append(ConvGRU_cell(ni, ni, ks=rnn_ks))\n",
    "        \n",
    "        #last layer\n",
    "        deconvs.append(ConvLayer(szs[-1], szs[-1], ks, padding=ks//2, act_cls=act, norm_type=norm))\n",
    "        self.deconvs = nn.ModuleList(TimeDistributed(conv) for conv in deconvs)\n",
    "        self.rnns = nn.ModuleList(rnns)\n",
    "        self.head = TimeDistributed(nn.Conv2d(szs[-1], n_out, kernel_size=1))\n",
    "\n",
    "    def forward_by_stage(self, inputs, state, deconv, rnn, side_in=None):\n",
    "        if self.debug: \n",
    "            print(f' Layer: {rnn}')\n",
    "            print(' inputs:, state: ', inputs.shape, state.shape)\n",
    "        inputs, state_stage = rnn(inputs, state)\n",
    "        if self.debug: \n",
    "            print(' after rnn: ', inputs.shape)\n",
    "            print(f' Layer: {deconv}')\n",
    "            print(f' before Upsample: {inputs.shape, side_in.shape}')\n",
    "        outputs_stage = deconv(inputs, side_in)\n",
    "        if self.debug: print(' after_deconvs: ', outputs_stage.shape)\n",
    "        return outputs_stage, state_stage\n",
    "    \n",
    "    def forward(self, dec_input, hidden_states, enc_outs):\n",
    "        enc_outs = [None]+enc_outs[:-1]\n",
    "        for i, (state, conv, rnn, enc_out) in enumerate(zip(hidden_states[::-1], self.deconvs, self.rnns, enc_outs[::-1])):\n",
    "            if self.debug: print(f'\\nStage: {i} ---------------------------------')\n",
    "            dec_input, state_stage = self.forward_by_stage(dec_input, state, conv, rnn, side_in=enc_out)\n",
    "        return self.head(dec_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (deconvs): ModuleList(\n",
       "    (0): TimeDistributed(UpsampleBLock(in=96, out=64, blur=False, residual=True, act=ReLU(), attn=False, norm=None))\n",
       "    (1): TimeDistributed(UpsampleBLock(in=64, out=16, blur=False, residual=True, act=ReLU(), attn=False, norm=None))\n",
       "    (2): TimeDistributed(ConvLayer(\n",
       "      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU()\n",
       "    ))\n",
       "  )\n",
       "  (rnns): ModuleList(\n",
       "    (0): ConvGRU_cell(in=96, out=96, ks=5)\n",
       "    (1): ConvGRU_cell(in=64, out=64, ks=5)\n",
       "  )\n",
       "  (head): TimeDistributed(Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1)))\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec = Decoder(debug=True)\n",
    "dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([2, 10, 16, 64, 64]),\n",
       " torch.Size([2, 10, 64, 32, 32]),\n",
       " torch.Size([2, 10, 96, 16, 16])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[_.shape for _ in enc_outs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stage: 0 ---------------------------------\n",
      " Layer: ConvGRU_cell(in=96, out=96, ks=5)\n",
      " inputs:, state:  torch.Size([2, 10, 96, 16, 16]) torch.Size([2, 96, 16, 16])\n",
      " after rnn:  torch.Size([2, 10, 96, 16, 16])\n",
      " Layer: TimeDistributed(UpsampleBLock(in=96, out=64, blur=False, residual=True, act=ReLU(), attn=False, norm=None))\n",
      " before Upsample: (torch.Size([2, 10, 96, 16, 16]), torch.Size([2, 10, 64, 32, 32]))\n",
      " after_deconvs:  torch.Size([2, 10, 64, 32, 32])\n",
      "\n",
      "Stage: 1 ---------------------------------\n",
      " Layer: ConvGRU_cell(in=64, out=64, ks=5)\n",
      " inputs:, state:  torch.Size([2, 10, 64, 32, 32]) torch.Size([2, 64, 32, 32])\n",
      " after rnn:  torch.Size([2, 10, 64, 32, 32])\n",
      " Layer: TimeDistributed(UpsampleBLock(in=64, out=16, blur=False, residual=True, act=ReLU(), attn=False, norm=None))\n",
      " before Upsample: (torch.Size([2, 10, 64, 32, 32]), torch.Size([2, 10, 16, 64, 64]))\n",
      " after_deconvs:  torch.Size([2, 10, 16, 64, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 1, 64, 64])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec(enc_outs[-1], h, enc_outs).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stage: 0 ---------------------------------\n",
      " Layer: ConvGRU_cell(in=96, out=96, ks=5)\n",
      " inputs:, state:  torch.Size([2, 10, 96, 16, 16]) torch.Size([2, 96, 16, 16])\n",
      " after rnn:  torch.Size([2, 10, 96, 16, 16])\n",
      " Layer: TimeDistributed(UpsampleBLock(in=96, out=64, blur=False, residual=True, act=ReLU(), attn=False, norm=None))\n",
      " before Upsample: (torch.Size([2, 10, 96, 16, 16]), torch.Size([2, 10, 64, 32, 32]))\n",
      " after_deconvs:  torch.Size([2, 10, 64, 32, 32])\n",
      "\n",
      "Stage: 1 ---------------------------------\n",
      " Layer: ConvGRU_cell(in=64, out=64, ks=5)\n",
      " inputs:, state:  torch.Size([2, 10, 64, 32, 32]) torch.Size([2, 64, 32, 32])\n",
      " after rnn:  torch.Size([2, 10, 64, 32, 32])\n",
      " Layer: TimeDistributed(UpsampleBLock(in=64, out=16, blur=False, residual=True, act=ReLU(), attn=False, norm=None))\n",
      " before Upsample: (torch.Size([2, 10, 64, 32, 32]), torch.Size([2, 10, 16, 64, 64]))\n",
      " after_deconvs:  torch.Size([2, 10, 16, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "test_eq(dec(enc_outs[-1], h, enc_outs).shape, imgs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _unbind_images(x, dim=1):\n",
    "    \"only unstack images\"\n",
    "    if isinstance(x, torch.Tensor): \n",
    "        if len(x.shape)>=4:\n",
    "            return x.unbind(dim=dim)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.rand(1,3,64,64)\n",
    "t2 = torch.rand(5)\n",
    "test_eq(_unbind_images(t), [t[:,i,...] for i in range(3)])\n",
    "test_eq(_unbind_images(t2), t2)\n",
    "test_eq(_unbind_images(5.0), 5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class StackUnstack(Module):\n",
    "    \"Stack together inputs, apply module, unstack output\"\n",
    "    def __init__(self, module, dim=1):\n",
    "        self.dim = dim\n",
    "        self.module = module\n",
    "    \n",
    "    @staticmethod\n",
    "    def unbind_images(x, dim=1): return _unbind_images(x, dim)\n",
    "    def forward(self, *args):\n",
    "        inputs = [torch.stack(x, dim=self.dim) for x in args]\n",
    "        outputs = self.module(*inputs)\n",
    "        if isinstance(outputs, (tuple, list)):\n",
    "            return [self.unbind_images(output, dim=self.dim) for output in outputs]\n",
    "        else: return outputs.unbind(dim=self.dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SimpleModel(Module):\n",
    "    \"Simple Encoder/Decoder module\"\n",
    "    def __init__(self, n_in=1, n_out=1, szs=[16,64,96], ks=3, rnn_ks=5, act=nn.ReLU, blur=False, attn=False, \n",
    "                 norm=None, strategy='zero', coord_conv=False, debug=False):\n",
    "        self.strategy = strategy\n",
    "        self.encoder = Encoder(n_in, szs, ks, rnn_ks, act, norm, coord_conv, debug)\n",
    "        self.decoder = Decoder(n_out, szs[::-1], ks, rnn_ks, act, blur, attn, norm, debug)\n",
    "    def forward(self, x):\n",
    "        enc_outs, h = self.encoder(x)\n",
    "        if self.strategy is 'zero':\n",
    "            dec_in = one_param(self).new_zeros(*enc_outs[-1].shape)\n",
    "        elif self.strategy is 'encoder':\n",
    "            dec_in = enc_outs[-1]\n",
    "        return self.decoder(dec_in, h, enc_outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = StackUnstack(SimpleModel(strategy='zero'))\n",
    "m2 = StackUnstack(SimpleModel(strategy='encoder'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_list = [torch.rand(2,1,64,64) for _ in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(len(m(imgs_list)), len(imgs_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the model can output a list of tensors, we will need to modify the loss function to acomodate this inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def StackLoss(loss_func=MSELossFlat(), axis=-1):\n",
    "    def _inner_loss(x,y):\n",
    "        x = torch.cat(x, axis)\n",
    "        y = torch.cat(y, axis)\n",
    "        return loss_func(x,y)\n",
    "    return _inner_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = StackLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 64, 640])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat(imgs_list, axis=-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 64, 640])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat(m(imgs_list), axis=-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0293, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func(imgs_list, m(imgs_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class MultiImageDice(Metric):\n",
    "    \"Dice coefficient metric for binary target in segmentation\"\n",
    "    def __init__(self, axis=1): self.axis = axis\n",
    "    def reset(self): self.inter,self.union = 0,0\n",
    "    def accumulate(self, learn):\n",
    "        x = torch.cat(learn.pred, -1)\n",
    "        y = torch.cat(learn.y, -1)\n",
    "#         print(type(x), type(y), x.shape, y.shape)\n",
    "        pred,targ = flatten_check(x.argmax(dim=self.axis), y)\n",
    "        self.inter += (pred*targ).float().sum().item()\n",
    "        self.union += (pred+targ).float().sum().item()\n",
    "\n",
    "    @property\n",
    "    def value(self): return 2. * self.inter/self.union if self.union > 0 else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_data.ipynb.\n",
      "Converted 01_models.conv_rnn.ipynb.\n",
      "Converted 02_models.dcn.ipynb.\n",
      "Converted 02_models.transformer.ipynb.\n",
      "Converted 02_tcn.ipynb.\n",
      "Converted 03_phy.ipynb.\n",
      "Converted 03_phy_original.ipynb.\n",
      "Converted 04_seq2seq.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
