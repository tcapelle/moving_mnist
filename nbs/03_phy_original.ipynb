{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.phy_original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PhyDNet original implementation\n",
    "> ConvLSTM + PhyCell\n",
    "https://github.com/vincent-leguen/PhyDNet/blob/master/models/models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai.vision.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quadro RTX 8000\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(0)\n",
    "    print(torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The PhyCell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- We will refactor this to not make the hidden state as a class attribute. We can also make use of some fastai magic, like `one_param` (to be sure to be on the same device as the model params) and `store_attr()` to save our class attributes. -->\n",
    "![phycell](images/phycell.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class PhyCell_Cell(Module):\n",
    "    def __init__(self, input_dim, F_hidden_dim, kernel_size, bias=True):\n",
    "        self.input_dim  = input_dim\n",
    "        self.F_hidden_dim = F_hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding     = kernel_size[0] // 2, kernel_size[1] // 2\n",
    "        self.bias = bias\n",
    "        \n",
    "        self.F = nn.Sequential()\n",
    "        self.F.add_module('bn1',nn.BatchNorm2d(input_dim))     \n",
    "#         self.F.add_module('bn1',nn.GroupNorm(4, input_dim))   \n",
    "        self.F.add_module('conv1', nn.Conv2d(in_channels=input_dim, out_channels=F_hidden_dim, kernel_size=self.kernel_size, stride=(1,1), padding=self.padding))  \n",
    "        self.F.add_module('f_act1', nn.LeakyReLU(negative_slope=0.1))        \n",
    "        self.F.add_module('conv2', nn.Conv2d(in_channels=F_hidden_dim, out_channels=input_dim, kernel_size=(1,1), stride=(1,1), padding=(0,0)))\n",
    "\n",
    "        self.convgate = nn.Conv2d(in_channels=self.input_dim + self.input_dim,\n",
    "                              out_channels= self.input_dim,\n",
    "                              kernel_size=(3,3),\n",
    "                              padding=(1,1), bias=self.bias)\n",
    "\n",
    "    def forward(self, x, hidden): # x [batch_size, hidden_dim, height, width] \n",
    "        hidden_tilde = hidden + self.F(hidden)        # prediction\n",
    "        \n",
    "        combined = torch.cat([x, hidden_tilde], dim=1)  # concatenate along channel axis\n",
    "        combined_conv = self.convgate(combined)\n",
    "        K = torch.sigmoid(combined_conv)\n",
    "        \n",
    "        next_hidden = hidden_tilde + K * (x-hidden_tilde)   # correction , Haddamard product     \n",
    "        return next_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhyCell_Cell(\n",
       "  (F): Sequential(\n",
       "    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (f_act1): LeakyReLU(negative_slope=0.1)\n",
       "    (conv2): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (convgate): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phy_cell = PhyCell_Cell(16, 32, (3, 3)).cuda()\n",
    "phy_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 16, 12, 12])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = phy_cell(torch.rand(64,16,12,12).cuda(), torch.rand(64,16,12,12).cuda())\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3360, device='cuda:0', grad_fn=<MseLossBackward>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_loss = MSELossFlat()\n",
    "loss = mse_loss(out, torch.zeros_like(out))\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3360, device='cuda:0', grad_fn=<MseLossBackward>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class PhyCell(Module):\n",
    "    def __init__(self, input_shape, input_dim, F_hidden_dims, n_layers, kernel_size):\n",
    "        self.input_shape = input_shape\n",
    "        self.input_dim = input_dim\n",
    "        self.F_hidden_dims = F_hidden_dims\n",
    "        self.n_layers = n_layers\n",
    "        self.kernel_size = kernel_size\n",
    "        self.H = []  \n",
    "             \n",
    "        cell_list = []\n",
    "        for i in range(0, self.n_layers):\n",
    "        #    cur_input_dim = self.input_dim if i == 0 else self.hidden_dims[i-1]\n",
    "\n",
    "            cell_list.append(PhyCell_Cell(input_dim=input_dim,\n",
    "                                          F_hidden_dim=self.F_hidden_dims[i],\n",
    "                                          kernel_size=self.kernel_size))                                     \n",
    "        self.cell_list = nn.ModuleList(cell_list)\n",
    "        \n",
    "       \n",
    "    def forward(self, input_, first_timestep=False): # input_ [batch_size, 1, channels, width, height]    \n",
    "        batch_size = input_.data.size()[0]\n",
    "        if (first_timestep):   \n",
    "            self.initHidden(batch_size, dtype=input_.dtype) # init Hidden at each forward start\n",
    "              \n",
    "        for j,cell in enumerate(self.cell_list):\n",
    "            if j==0: # bottom layer\n",
    "                self.H[j] = cell(input_, self.H[j])\n",
    "            else:\n",
    "                self.H[j] = cell(self.H[j-1],self.H[j])\n",
    "        \n",
    "        return self.H , self.H \n",
    "    \n",
    "    def initHidden(self, batch_size, dtype):\n",
    "        self.H = [] \n",
    "        for i in range(self.n_layers):\n",
    "            self.H.append(one_param(self).new_zeros(batch_size, self.input_dim, self.input_shape[0], self.input_shape[1], dtype=dtype) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phy = PhyCell((6,6), 8, [8,8], n_layers=2, kernel_size=(3,3)).cuda()\n",
    "out, states = phy(torch.rand(1,8,6,6).cuda(), True)\n",
    "out = torch.stack(out, dim=1)\n",
    "states = torch.stack(states, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 2, 8, 6, 6]), torch.Size([2, 1, 8, 6, 6]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape, states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0715, device='cuda:0', grad_fn=<MseLossBackward>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_loss = MSELossFlat()\n",
    "loss = mse_loss(out, torch.zeros_like(out))\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0715, device='cuda:0', grad_fn=<MseLossBackward>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ConvLSTM_Cell(nn.Module):\n",
    "    def __init__(self, input_shape, input_dim, hidden_dim, kernel_size, bias=1):              \n",
    "        \"\"\"\n",
    "        input_shape: (int, int)\n",
    "            Height and width of input tensor as (height, width).\n",
    "        input_dim: int\n",
    "            Number of channels of input tensor.\n",
    "        hidden_dim: int\n",
    "            Number of channels of hidden state.\n",
    "        kernel_size: (int, int)\n",
    "            Size of the convolutional kernel.\n",
    "        bias: bool\n",
    "            Whether or not to add the bias.\n",
    "        \"\"\"\n",
    "        super(ConvLSTM_Cell, self).__init__()\n",
    "        \n",
    "        self.height, self.width = input_shape\n",
    "        self.input_dim  = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding     = kernel_size[0] // 2, kernel_size[1] // 2\n",
    "        self.bias        = bias\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,\n",
    "                              out_channels=4 * self.hidden_dim,\n",
    "                              kernel_size=self.kernel_size,\n",
    "                              padding=self.padding, bias=self.bias)\n",
    "                 \n",
    "    # we implement LSTM that process only one timestep \n",
    "    def forward(self,x, hidden): # x [batch, hidden_dim, width, height]          \n",
    "        h_cur, c_cur = hidden\n",
    "        \n",
    "        combined = torch.cat([x, h_cur], dim=1)  # concatenate along channel axis\n",
    "        combined_conv = self.conv(combined)\n",
    "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1) \n",
    "        i = torch.sigmoid(cc_i)\n",
    "        f = torch.sigmoid(cc_f)\n",
    "        o = torch.sigmoid(cc_o)\n",
    "        g = torch.tanh(cc_g)\n",
    "\n",
    "        c_next = f * c_cur + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "        return h_next, c_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_cell = ConvLSTM_Cell((6,6), 8, 8, kernel_size=(3,3)).cuda()\n",
    "h, c = conv_cell(torch.rand(1,8,6,6).cuda(), (torch.rand(1,8,6,6).cuda(), torch.rand(1,8,6,6).cuda()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ConvLSTM(nn.Module):\n",
    "    def __init__(self, input_shape, input_dim, hidden_dims, n_layers, kernel_size):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.n_layers = n_layers\n",
    "        self.kernel_size = kernel_size\n",
    "        self.H, self.C = [],[]   \n",
    "        \n",
    "        cell_list = []\n",
    "        for i in range(0, self.n_layers):\n",
    "            cur_input_dim = self.input_dim if i == 0 else self.hidden_dims[i-1]\n",
    "            print('layer ',i,'input dim ', cur_input_dim, ' hidden dim ', self.hidden_dims[i])\n",
    "            cell_list.append(ConvLSTM_Cell(input_shape=self.input_shape,\n",
    "                                          input_dim=cur_input_dim,\n",
    "                                          hidden_dim=self.hidden_dims[i],\n",
    "                                          kernel_size=self.kernel_size))                                     \n",
    "        self.cell_list = nn.ModuleList(cell_list)\n",
    "        \n",
    "       \n",
    "    def forward(self, input_, first_timestep=False): # input_ [batch_size, channels, width, height]    \n",
    "        batch_size = input_.data.size()[0]\n",
    "        if (first_timestep):   \n",
    "            self.initHidden(batch_size) # init Hidden at each forward start\n",
    "              \n",
    "        for j,cell in enumerate(self.cell_list):\n",
    "            if j==0: # bottom layer\n",
    "                self.H[j], self.C[j] = cell(input_, (self.H[j],self.C[j]))\n",
    "            else:\n",
    "                self.H[j], self.C[j] = cell(self.H[j-1],(self.H[j],self.C[j]))\n",
    "        \n",
    "        return (self.H,self.C) , self.H   # (hidden, output)\n",
    "    \n",
    "    def initHidden(self,batch_size):\n",
    "        self.H, self.C = [],[]  \n",
    "        for i in range(self.n_layers):\n",
    "            self.H.append( one_param(self).new_zeros(batch_size,self.hidden_dims[i], self.input_shape[0], self.input_shape[1]) )\n",
    "            self.C.append( one_param(self).new_zeros(batch_size,self.hidden_dims[i], self.input_shape[0], self.input_shape[1]) )\n",
    "    \n",
    "    def setHidden(self, hidden):\n",
    "        H,C = hidden\n",
    "        self.H, self.C = H,C\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer  0 input dim  8  hidden dim  8\n"
     ]
    }
   ],
   "source": [
    "conv = ConvLSTM((6,6), 8, [8], n_layers=1, kernel_size=(3,3)).cuda()\n",
    "h, c = conv(torch.rand(1,8,6,6).cuda(), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class dcgan_conv(nn.Module):\n",
    "    def __init__(self, nin, nout, stride):\n",
    "        super(dcgan_conv, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=nin, out_channels=nout, kernel_size=(3,3), stride=stride, padding=1),\n",
    "                nn.GroupNorm(4,nout),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "        \n",
    "class dcgan_upconv(nn.Module):\n",
    "    def __init__(self, nin, nout, stride):\n",
    "        super(dcgan_upconv, self).__init__()\n",
    "        if (stride ==2):\n",
    "            output_padding = 1\n",
    "        else:\n",
    "            output_padding = 0\n",
    "        self.main = nn.Sequential(\n",
    "                nn.ConvTranspose2d(in_channels=nin,out_channels=nout,kernel_size=(3,3), stride=stride,padding=1,output_padding=output_padding),\n",
    "                nn.GroupNorm(4,nout),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class image_encoder(nn.Module):\n",
    "    def __init__(self, nc=1):\n",
    "        super(image_encoder, self).__init__()\n",
    "        nf = 16\n",
    "        # input is (nc) x 64 x 64\n",
    "        self.c1 = dcgan_conv(nc, int(nf/2), stride=1) # (nf) x 64 x 64\n",
    "        self.c2 = dcgan_conv(int(nf/2), nf, stride=1) # (nf) x 64 x 64\n",
    "        self.c3 = dcgan_conv(nf, nf*2, stride=2) # (2*nf) x 32 x 32\n",
    "        self.c4 = dcgan_conv(nf*2, nf*2, stride=1) # (2*nf) x 32 x 32\n",
    "        self.c5 = dcgan_conv(nf*2, nf*4, stride=2) # (4*nf) x 16 x 16\n",
    "        self.c6 = dcgan_conv(nf*4, nf*4, stride=1) # (4*nf) x 16 x 16          \n",
    "\n",
    "    def forward(self, input):\n",
    "        h1 = self.c1(input)  # (nf/2) x 64 x 64\n",
    "        h2 = self.c2(h1)     # (nf) x 64 x 64\n",
    "        h3 = self.c3(h2)     # (2*nf) x 32 x 32\n",
    "        h4 = self.c4(h3)     # (2*nf) x 32 x 32\n",
    "        h5 = self.c5(h4)     # (4*nf) x 16 x 16\n",
    "        h6 = self.c6(h5)     # (4*nf) x 16 x 16          \n",
    "        return h6, [h1, h2, h3, h4, h5, h6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_encoder = image_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class image_decoder(nn.Module):\n",
    "    def __init__(self, nc=1):\n",
    "        super(image_decoder, self).__init__()\n",
    "        nf = 16\n",
    "        self.upc1 = dcgan_upconv(nf*4*2, nf*4, stride=1) #(nf*4) x 16 x 16\n",
    "        self.upc2 = dcgan_upconv(nf*4*2, nf*2, stride=2) #(nf*2) x 32 x 32\n",
    "        self.upc3 = dcgan_upconv(nf*2*2, nf*2, stride=1) #(nf*2) x 32 x 32\n",
    "        self.upc4 = dcgan_upconv(nf*2*2, nf, stride=2)   #(nf) x 64 x 64\n",
    "        self.upc5 = dcgan_upconv(nf*2, int(nf/2), stride=1)   #(nf/2) x 64 x 64\n",
    "        self.upc6 = nn.ConvTranspose2d(in_channels=nf,out_channels=nc,kernel_size=(3,3),stride=1,padding=1)  #(nc) x 64 x 64\n",
    "\n",
    "    def forward(self, input):\n",
    "        vec, skip = input    # vec: (4*nf) x 16 x 16          \n",
    "        [h1, h2, h3, h4, h5, h6] = skip\n",
    "        d1 = self.upc1(torch.cat([vec, h6], dim=1))  #(nf*4) x 16 x 16\n",
    "        d2 = self.upc2(torch.cat([d1, h5], dim=1))   #(nf*2) x 32 x 32\n",
    "        d3 = self.upc3(torch.cat([d2, h4], dim=1))   #(nf*2) x 32 x 32\n",
    "        d4 = self.upc4(torch.cat([d3, h3], dim=1))   #(nf) x 64 x 64\n",
    "        d5 = self.upc5(torch.cat([d4, h2], dim=1))   #(nf/2) x 64 x 64\n",
    "        d6 = self.upc6(torch.cat([d5, h1], dim=1))   #(nc) x 64 x 64\n",
    "        return d6\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_decoder = image_decoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class EncoderRNN(torch.nn.Module):\n",
    "    def __init__(self,phycell,convlstm, ):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.image_cnn_enc = image_encoder() # image encoder 64x64x1 -> 16x16x64\n",
    "        self.image_cnn_dec = image_decoder() # image decoder 16x16x64 -> 64x64x1 \n",
    "        \n",
    "        self.phycell = phycell\n",
    "        self.convlstm = convlstm\n",
    "\n",
    "    def forward(self, input, first_timestep=False):\n",
    "        encoded_image, skip = self.image_cnn_enc(input)\n",
    "        \n",
    "        hidden1, output1 = self.phycell(encoded_image, first_timestep)\n",
    "        hidden2, output2 = self.convlstm(encoded_image, first_timestep)\n",
    "        \n",
    "        concat = output1[-1] + output2[-1]\n",
    "        \n",
    "        output_image =  self.image_cnn_dec([concat,skip]) \n",
    "        return output_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer  0 input dim  64  hidden dim  128\n",
      "layer  1 input dim  128  hidden dim  128\n",
      "layer  2 input dim  128  hidden dim  64\n"
     ]
    }
   ],
   "source": [
    "phycell = PhyCell(input_shape=(16,16), input_dim=64, F_hidden_dims=[49], n_layers=1, kernel_size=(7,7)) \n",
    "convlstm = ConvLSTM(input_shape=(16,16), input_dim=64, hidden_dims=[128,128,64], n_layers=3, kernel_size=(3,3))   \n",
    "encoder = EncoderRNN(phycell, convlstm).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 64, 64])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder(torch.rand(1,1,64,64).cuda(), True).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "very horrible imports!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from numpy import *\n",
    "from numpy.linalg import *\n",
    "from scipy.special import factorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _apply_axis_left_dot(x, mats):\n",
    "    assert x.dim() == len(mats)+1\n",
    "    sizex = x.size()\n",
    "    k = x.dim()-1\n",
    "    for i in range(k):\n",
    "        x = tensordot(mats[k-i-1], x, dim=[1,k])\n",
    "    x = x.permute([k,]+list(range(k))).contiguous()\n",
    "    x = x.view(sizex)\n",
    "    return x\n",
    "\n",
    "def _apply_axis_right_dot(x, mats):\n",
    "    assert x.dim() == len(mats)+1\n",
    "    sizex = x.size()\n",
    "    k = x.dim()-1\n",
    "    x = x.permute(list(range(1,k+1))+[0,])\n",
    "    for i in range(k):\n",
    "        x = tensordot(x, mats[i], dim=[0,0])\n",
    "    x = x.contiguous()\n",
    "    x = x.view(sizex)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class _MK(nn.Module):\n",
    "    def __init__(self, shape):\n",
    "        super(_MK, self).__init__()\n",
    "        self._size = torch.Size(shape)\n",
    "        self._dim = len(shape)\n",
    "        M = []\n",
    "        invM = []\n",
    "        assert len(shape) > 0\n",
    "        j = 0\n",
    "        for l in shape:\n",
    "            M.append(zeros((l,l)))\n",
    "            for i in range(l):\n",
    "                M[-1][i] = ((arange(l)-(l-1)//2)**i)/factorial(i)\n",
    "            invM.append(inv(M[-1]))\n",
    "            self.register_buffer('_M'+str(j), torch.from_numpy(M[-1]))\n",
    "            self.register_buffer('_invM'+str(j), torch.from_numpy(invM[-1]))\n",
    "            j += 1\n",
    "\n",
    "    @property\n",
    "    def M(self):\n",
    "        return list(self._buffers['_M'+str(j)] for j in range(self.dim()))\n",
    "    @property\n",
    "    def invM(self):\n",
    "        return list(self._buffers['_invM'+str(j)] for j in range(self.dim()))\n",
    "\n",
    "    def size(self):\n",
    "        return self._size\n",
    "    def dim(self):\n",
    "        return self._dim\n",
    "    def _packdim(self, x):\n",
    "        assert x.dim() >= self.dim()\n",
    "        if x.dim() == self.dim():\n",
    "            x = x[newaxis,:]\n",
    "        x = x.contiguous()\n",
    "        x = x.view([-1,]+list(x.size()[-self.dim():]))\n",
    "        return x\n",
    "\n",
    "    def forward(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = _MK((7,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class M2K(_MK):\n",
    "    \"\"\"\n",
    "    convert moment matrix to convolution kernel\n",
    "    Arguments:\n",
    "        shape (tuple of int): kernel shape\n",
    "    Usage:\n",
    "        m2k = M2K([5,5])\n",
    "        m = torch.randn(5,5,dtype=torch.float64)\n",
    "        k = m2k(m)\n",
    "    \"\"\"\n",
    "    def __init__(self, shape):\n",
    "        super(M2K, self).__init__(shape)\n",
    "    def forward(self, m):\n",
    "        \"\"\"\n",
    "        m (Tensor): torch.size=[...,*self.shape]\n",
    "        \"\"\"\n",
    "        sizem = m.size()\n",
    "        m = self._packdim(m)\n",
    "        m = _apply_axis_left_dot(m, self.invM)\n",
    "        m = m.view(sizem)\n",
    "        return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class K2M(_MK):\n",
    "    \"\"\"\n",
    "    convert convolution kernel to moment matrix\n",
    "    Arguments:\n",
    "        shape (tuple of int): kernel shape\n",
    "    Usage:\n",
    "        k2m = K2M([5,5])\n",
    "        k = torch.randn(5,5,dtype=torch.float64)\n",
    "        m = k2m(k)\n",
    "    \"\"\"\n",
    "    def __init__(self, shape):\n",
    "        super(K2M, self).__init__(shape)\n",
    "    def forward(self, k):\n",
    "        \"\"\"\n",
    "        k (Tensor): torch.size=[...,*self.shape]\n",
    "        \"\"\"\n",
    "        sizek = k.size()\n",
    "        k = self._packdim(k)\n",
    "        k = _apply_axis_left_dot(k, self.M)\n",
    "        k = k.view(sizek)\n",
    "        return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export    \n",
    "def tensordot(a,b,dim):\n",
    "    \"\"\"\n",
    "    tensordot in PyTorch, see numpy.tensordot?\n",
    "    \"\"\"\n",
    "    l = lambda x,y:x*y\n",
    "    if isinstance(dim,int):\n",
    "        a = a.contiguous()\n",
    "        b = b.contiguous()\n",
    "        sizea = a.size()\n",
    "        sizeb = b.size()\n",
    "        sizea0 = sizea[:-dim]\n",
    "        sizea1 = sizea[-dim:]\n",
    "        sizeb0 = sizeb[:dim]\n",
    "        sizeb1 = sizeb[dim:]\n",
    "        N = reduce(l, sizea1, 1)\n",
    "        assert reduce(l, sizeb0, 1) == N\n",
    "    else:\n",
    "        adims = dim[0]\n",
    "        bdims = dim[1]\n",
    "        adims = [adims,] if isinstance(adims, int) else adims\n",
    "        bdims = [bdims,] if isinstance(bdims, int) else bdims\n",
    "        adims_ = set(range(a.dim())).difference(set(adims))\n",
    "        adims_ = list(adims_)\n",
    "        adims_.sort()\n",
    "        perma = adims_+adims\n",
    "        bdims_ = set(range(b.dim())).difference(set(bdims))\n",
    "        bdims_ = list(bdims_)\n",
    "        bdims_.sort()\n",
    "        permb = bdims+bdims_\n",
    "        a = a.permute(*perma).contiguous()\n",
    "        b = b.permute(*permb).contiguous()\n",
    "\n",
    "        sizea = a.size()\n",
    "        sizeb = b.size()\n",
    "        sizea0 = sizea[:-len(adims)]\n",
    "        sizea1 = sizea[-len(adims):]\n",
    "        sizeb0 = sizeb[:len(bdims)]\n",
    "        sizeb1 = sizeb[len(bdims):]\n",
    "        N = reduce(l, sizea1, 1)\n",
    "        assert reduce(l, sizeb0, 1) == N\n",
    "    a = a.view([-1,N])\n",
    "    b = b.view([N,-1])\n",
    "    c = a@b\n",
    "    return c.view(sizea0+sizeb1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PhyDNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class PhyDNet(Module):\n",
    "    def __init__(self, encoder, criterion=MSELossFlat(), sigmoid=False, moment=True): \n",
    "        store_attr()\n",
    "        self.pr = 0\n",
    "        self.k2m = K2M([7,7])\n",
    "        self.constraints = torch.zeros((49,7,7))\n",
    "        ind = 0\n",
    "        for i in range(0,7):\n",
    "            for j in range(0,7):\n",
    "                self.constraints[ind,i,j] = 1\n",
    "                ind +=1  \n",
    "\n",
    "    def forward(self, input_tensor, target_tensor=None):\n",
    "        device = one_param(self).device\n",
    "        \n",
    "        input_length  = input_tensor.size(1)\n",
    "        target_length = target_tensor.size(1)\n",
    "        loss = 0.\n",
    "        for ei in range(input_length-1): \n",
    "            output_image = self.encoder(input_tensor[:,ei,:,:,:], (ei==0) )\n",
    "            loss += self.criterion(output_image, input_tensor[:,ei+1,:,:,:])\n",
    "        \n",
    "        decoder_input = input_tensor[:,-1,:,:,:] # first decoder input = last image of input sequence\n",
    "        \n",
    "        output_images = []\n",
    "        if (target_tensor is not None) and (random.random()<self.pr):\n",
    "            for di in range(target_length):\n",
    "                output_image = self.encoder(decoder_input)\n",
    "                output_images.append(output_image)\n",
    "                decoder_input = target_tensor[:,di,:,:,:]\n",
    "        else:\n",
    "            for di in range(target_length):\n",
    "                output_image = self.encoder(decoder_input)\n",
    "                decoder_input = output_image\n",
    "                output_images.append(output_image)\n",
    "                \n",
    "        # Moment Regularisation  encoder.phycell.cell_list[0].F.conv1.weight # size (nb_filters,in_channels,7,7)\n",
    "        if self.moment:\n",
    "            for b in range(0,self.encoder.phycell.cell_list[0].input_dim):\n",
    "                filters = self.encoder.phycell.cell_list[0].F.conv1.weight[:,b,:,:] # (nb_filters,7,7)\n",
    "                m = self.k2m(filters.double()) \n",
    "                m  = m.float()   \n",
    "                loss += self.criterion(m, self.constraints.to(device)) # constrains is a precomputed matrix   \n",
    "            \n",
    "        out_images = torch.stack(output_images, dim=1)\n",
    "        out_images = torch.sigmoid(out_images) if self.sigmoid else out_images\n",
    "        \n",
    "        return out_images, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phynet = PhyDNet(encoder).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, loss = phynet(torch.rand(1,5,1,64,64).cuda(), target_tensor=torch.rand(1,5,1,64,64).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 5, 1, 64, 64]),\n",
       " tensor(11.4649, device='cuda:0', grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_data.ipynb.\n",
      "Converted 01_models.conv_rnn.ipynb.\n",
      "Converted 02_models.dcn.ipynb.\n",
      "Converted 02_models.transformer.ipynb.\n",
      "Converted 02_tcn.ipynb.\n",
      "Converted 03_phy.ipynb.\n",
      "Converted 03_phy_original.ipynb.\n",
      "Converted 04_seq2seq.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
